{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Name - Vijay Iyer (vsiyer)\n",
    "###  Course - DSCI 590 (INTRO TO NLP FOR DATA SCIENCE)\n",
    "### Title  - Question Answering Chatbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import random as rnd\n",
    "import colorama\n",
    "from colorama import Fore, Style, Back\n",
    "from termcolor import colored, cprint\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - Preprocessing SQUAD Dataset\n",
    " - We will be working on SQUAD v1.1 dataset - SQuAD is a collection of 100,00 crowdsourced question-answer pairs. It's also used as a benchmark for evaluating model performance on question answering NLP task\n",
    " - the dataset comes in 2 files a ***train .json*** and a ***dev .json file***\n",
    " - train dataset has **87,599 question-answer pairs while dev dataset has 10,570**\n",
    " - there are in total **442 different articles** (from which 87,599 context-question-answer combinations have been extracted) the format of the data is in the form  - title, [list of **'context'** paragraphs, list of question-answer(multiple possible answers per question) pairs for each 'context'] \n",
    " - the 'source' is the pair of context and question with the answer being the 'target' for that particular pair of context and question. A machine learning model will have to train on context and question data with answer as target to then be able to answer questions from unseen articles/ paragraphs\n",
    " - for training for chatbot, we will have to get each context-question-answer combination as a seperate record. \n",
    " - for this assignment, however, we will be doing the preprocessing and transforms on the **'context'** column, since that's the one having unstructured text data\n",
    " - we will download the json content using requests module\n",
    " - After downloading the json files, we will convert the content to a suitable dataframe format from where training and evaluation can be done. ---> **url = https://rajpurkar.github.io/SQuAD-explorer/dataset/**\n",
    " - we can see these steps (for part 1) below\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
    "train_url = os.path.join(url, \"train-v1.1.json\")\n",
    "dev_url = os.path.join(url, \"dev-v1.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(train_url)\n",
    "train_json = json.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(dev_url)\n",
    "dev_json = json.loads(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis on train and dev json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "print(len(train_json['data'])) #  -- number of titles in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_json['data'])) # -- number of titles in dev dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print sample record to see the structure of a data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_json['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 442 different records.\n",
    "Each record has a title and a list of \n",
    "1. paragraphs - which each consist of context, questions and answers\n",
    "2. Context is an article or the tet on which questions will be asked\n",
    "3. Questions\n",
    "4. Multiple Answers for each question - these are the ground truth or correct answers for the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions to convert the text to dataframe with format\n",
    " - id, title, Question, Context, answer_start (since answer can be found within context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_train(train_json, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "      \n",
    "    file = train_json\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.json_normalize(file , record_path )\n",
    "    m = pd.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    js['q_idx'] = ndx\n",
    "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_dev(dev_json, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "       \n",
    "    file = dev_json\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.json_normalize(file , record_path )\n",
    "    m = pd.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "#     js['q_idx'] = ndx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert json to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing...\n",
      "shape of the dataframe is (87599, 6)\n",
      "Done\n",
      "processing...\n",
      "shape of the dataframe is (10570, 5)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "record_path = ['data','paragraphs','qas','answers']\n",
    "train_df = squad_json_to_dataframe_train(train_json,record_path=record_path)\n",
    "dev_df = squad_json_to_dataframe_dev(dev_json,record_path=record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      index  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "\n",
       "                                             context  answer_start  \\\n",
       "0  Architecturally, the school has a Catholic cha...           515   \n",
       "1  Architecturally, the school has a Catholic cha...           188   \n",
       "2  Architecturally, the school has a Catholic cha...           279   \n",
       "\n",
       "                         text  c_id  \n",
       "0  Saint Bernadette Soubirous     0  \n",
       "1   a copper statue of Christ     0  \n",
       "2           the Main Building     0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be4db0acb8001400a502ec</td>\n",
       "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>[{'answer_start': 177, 'text': 'Denver Broncos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be4db0acb8001400a502ed</td>\n",
       "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>[{'answer_start': 249, 'text': 'Carolina Panth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be4db0acb8001400a502ee</td>\n",
       "      <td>Where did Super Bowl 50 take place?</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>[{'answer_start': 403, 'text': 'Santa Clara, C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  56be4db0acb8001400a502ec   \n",
       "1  56be4db0acb8001400a502ed   \n",
       "2  56be4db0acb8001400a502ee   \n",
       "\n",
       "                                            question  \\\n",
       "0  Which NFL team represented the AFC at Super Bo...   \n",
       "1  Which NFL team represented the NFC at Super Bo...   \n",
       "2                Where did Super Bowl 50 take place?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Super Bowl 50 was an American football game to...   \n",
       "1  Super Bowl 50 was an American football game to...   \n",
       "2  Super Bowl 50 was an American football game to...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0  [{'answer_start': 177, 'text': 'Denver Broncos...     0  \n",
       "1  [{'answer_start': 249, 'text': 'Carolina Panth...     0  \n",
       "2  [{'answer_start': 403, 'text': 'Santa Clara, C...     0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**getting BertTokenizer which is pre-trained which is fine-tuned on SQuAD dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see how the input text is tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = np.random.randint(0,len(dev_df))\n",
    "question = dev_df[\"question\"][random_num]\n",
    "context = dev_df[\"context\"][random_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **encode_plus** function to tokenize the question-context pair. Bert needs input in the form of token embeddings as well as segment embeddings which can help it differentiate between either a pair of sentences or question and text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids'])\n",
      "The input has a total of 258 tokens.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode_plus(question, context)\n",
    "print(input_ids.keys())\n",
    "print(\"The input has a total of {} tokens.\".format(len(input_ids['input_ids'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens with their ids. **Some of the tokens are printed out with *##* at the beginning**, this is because BERT does wordpiece tokenization on the words, in order to reduce the size of the vocabulary and so unseen words can be represented. Words are broken into sub-wods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what ,did ,luther ,try ,to ,do ,for ,the ,jews ,? ,luther ,wrote ,about ,the ,jews ,throughout ,his ,career ,, ,though ,only ,a ,few ,of ,his ,works ,dealt ,with ,them ,directly ,. ,luther ,rarely ,encountered ,jews ,during ,his ,life ,, ,but ,his ,attitudes ,reflected ,a ,theological ,and ,cultural ,tradition ,which ,saw ,jews ,as ,a ,rejected ,people ,guilty ,of ,the ,murder ,of ,christ ,, ,and ,he ,lived ,within ,a ,local ,community ,that ,had ,expelled ,jews ,some ,ninety ,years ,earlier ,. ,he ,considered ,the ,jews ,b ,##las ,##ph ,##eme ,##rs ,and ,liar ,##s ,because ,they ,rejected ,the ,divinity ,of ,jesus ,, ,whereas ,christians ,believed ,jesus ,was ,the ,messiah ,. ,but ,luther ,believed ,that ,all ,human ,beings ,who ,set ,themselves ,against ,god ,were ,equally ,guilty ,. ,as ,early ,as ,151 ,##6 ,, ,he ,wrote ,that ,many ,people ,\" ,are ,proud ,with ,marvelous ,stupidity ,when ,they ,call ,the ,jews ,dogs ,, ,evil ,##do ,##ers ,, ,or ,whatever ,they ,like ,, ,while ,they ,too ,, ,and ,equally ,, ,do ,not ,realize ,who ,or ,what ,they ,are ,in ,the ,sight ,of ,god ,\" ,. ,in ,152 ,##3 ,, ,luther ,advised ,kindness ,toward ,the ,jews ,in ,that ,jesus ,christ ,was ,born ,a ,jew ,and ,also ,aimed ,to ,convert ,them ,to ,christianity ,. ,when ,his ,efforts ,at ,conversion ,failed ,, ,he ,grew ,increasingly ,bitter ,toward ,them ,. ,in ,his ,2010 ,book ,bon ,##hoe ,##ffer ,: ,pastor ,, ,martyr ,, ,prophet ,, ,spy ,, ,christian ,author ,eric ,meta ,##xa ,##s ,claimed ,that ,luther ,' ,s ,attitude ,towards ,jews ,\" ,un ,##rave ,##led ,along ,with ,his ,health ,. ,\"\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids['input_ids'])\n",
    "print(' ,'.join(tokens))\n",
    "## uncomment below line to see all tokens and their ids\n",
    "# for token, id in zip(tokens, input_ids['input_ids']):\n",
    "#     print('{:8}{:8,}'.format(token,id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    " - We will be using HuggingFace Transformers library\n",
    " - It's a pytorch implementation of BERT\n",
    " - It provides pre-trained models for NLP tasks, like Sentence Classification, Question Answering, Text Summarization, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting pre-trained models fine-tuned on SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since model is **pre-trained**, we will be directly using it for predicting answer, which is to predict start and end span of the answer from the context given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.tensor([input_ids['input_ids']]),  token_type_ids=torch.tensor([input_ids['token_type_ids']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "What did luther try to do for the jews?\n",
      "\n",
      "Answer:\n",
      "Convert them to christianity.\n"
     ]
    }
   ],
   "source": [
    "#tokens with highest start and end scores\n",
    "answer_start = torch.argmax(output[0]) # the token with highest probability of being start of the answer\n",
    "answer_end = torch.argmax(output[1]) # the token with highest probability of being end of the answer\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "    \n",
    "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minor modification is required to above code for mode readability. If the tokens are returned where one of the tokens is ***##{word}***, then we can prepend the remaining text in token to the token before, since in wordpiece tokenization performed by BERT, the start of a word can never be ***##**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokens[answer_start]\n",
    "for i in range(answer_start+1, answer_end+1):\n",
    "    if tokens[i][0:2] == \"##\":\n",
    "        answer += tokens[i][2:]\n",
    "    else:\n",
    "        answer += \" \" + tokens[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'convert them to christianity'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    " - We will convert the above function into a function which picks a passage and corresponding question about the passage. \n",
    " - We will also see how the model does for questions which cannot be answered from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, passage):\n",
    "    \n",
    "    #tokenize question and text as a pair\n",
    "    inputs = tokenizer.encode_plus(question, passage)\n",
    "    \n",
    "    input_ids, token_type_ids = inputs['input_ids'], inputs['token_type_ids']\n",
    "    #string version of tokenized ids\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    #model output using input_ids and segment_ids\n",
    "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "    \n",
    "    #reconstructing the answer\n",
    "    answer_start = torch.argmax(output[0])\n",
    "    answer_end = torch.argmax(output[1])\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    elif answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    else:\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run below cell repeatedly to see different context, question, answer pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "How many things did Microsoft alter after Release Preview?\u001b[0m\n",
      "\n",
      "Relatively few changes were made from the Release Preview to the final version; these included updated versions of its pre-loaded apps, the renaming of Windows Explorer to File Explorer, the replacement of the Aero Glass theme from Windows Vista and 7 with a new flat and solid-colored theme, and the addition of new background options for the Start screen, lock screen, and desktop. Prior to its general availability on October 26, 2012, updates were released for some of Windows 8's bundled apps, and a \"General Availability Cumulative Update\" (which included fixes to improve performance, compatibility, and battery life) was released on Tuesday, October 9, 2012. Microsoft indicated that due to improvements to its testing infrastructure, general improvements of this nature are to be released more frequently through Windows Update instead of being relegated to OEMs and service packs only.\u001b[0m\n",
      "\u001b[1m\n",
      "Predicted answer:\u001b[0m\u001b[1m\u001b[32mrelatively few\u001b[0m\n",
      "\u001b[1m\n",
      "Original answer:\u001b[0m\u001b[1m\u001b[32mRelatively few\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ind = rnd.randrange(len(train_df))\n",
    "passage = train_df.iloc[ind]['context']\n",
    "question = train_df.iloc[ind]['question']\n",
    "ground_truth_ans = train_df.iloc[ind]['text']\n",
    "\n",
    "answer = question_answer(question = question, passage = passage)\n",
    "cprint(\"\\n\"+question, attrs = ['bold'])\n",
    "cprint(\"\\n\"+passage)\n",
    "print(colored(\"\\nPredicted answer:\", attrs=['bold'])+colored(answer, 'green',attrs=['bold']))\n",
    "#original answer from the dataset\n",
    "print(colored(\"\\nOriginal answer:\", attrs = ['bold'])+colored(ground_truth_ans, 'green',attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or see the cell below it to give your own passage and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your text: \n",
      "Relatively few changes were made from the Release Preview to the final version; these included updated versions of its pre-loaded apps, the renaming of Windows Explorer to File Explorer, the replacement of the Aero Glass theme from Windows Vista and 7 with a new flat and solid-colored theme, and the addition of new background options for the Start screen, lock screen, and desktop. Prior to its general availability on October 26, 2012, updates were released for some of Windows 8's bundled apps, and a \"General Availability Cumulative Update\" (which included fixes to improve performance, compatibility, and battery life) was released on Tuesday, October 9, 2012. Microsoft indicated that due to improvements to its testing infrastructure, general improvements of this nature are to be released more frequently through Windows Update instead of being relegated to OEMs and service packs only.\n",
      "\n",
      "Please enter your question: \n",
      "What was renamed?\n",
      "\u001b[1m\n",
      "windows explorer to file explorer\u001b[0m\n",
      "\n",
      "Do you want to ask another question based on this text (Y/N)? What were the changes from preview to final version?\n",
      "\n",
      "Do you want to ask another question based on this text (Y/N)? Y\n",
      "\n",
      "Please enter your question: \n",
      "What were the changes from preview to final version?\n",
      "\u001b[1m\n",
      "relatively few\u001b[0m\n",
      "\n",
      "Do you want to ask another question based on this text (Y/N)? N\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "context = input(\"Please enter your text: \\n\")\n",
    "question = input(\"\\nPlease enter your question: \\n\")\n",
    "while True:\n",
    "    answer = question_answer(question, context)\n",
    "    cprint(\"\\n\"+answer, attrs=['bold'])\n",
    "    flag = True\n",
    "    flag_N = False\n",
    "    \n",
    "    while flag:\n",
    "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
    "        if response[0] == \"Y\":\n",
    "            question = input(\"\\nPlease enter your question: \\n\")\n",
    "            flag = False\n",
    "        elif response[0] == \"N\":\n",
    "            print(\"\\nBye!\")\n",
    "            flag = False\n",
    "            flag_N = True\n",
    "            \n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
